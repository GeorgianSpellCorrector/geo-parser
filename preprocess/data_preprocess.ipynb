{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add custom concatenation of txt files controlled by some config to avoid saturation of topics\n",
    "# this is temporary\n",
    "\n",
    "TEXT_FILE_PATH = '/home/penguin/GeorgianWritingWizard/data/geo_corpus.txt'\n",
    "OUT_TEXT_FILE_PATH = '/home/penguin/GeorgianWritingWizard/data/stripped_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_FILE_PATH, 'r', encoding='utf-8') as f, \\\n",
    "     open(OUT_TEXT_FILE_PATH, 'w', encoding='utf-8') as out:\n",
    "    for line in f:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        out.write(unicodedata.normalize(\"NFKD\", line.strip()) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = '/home/penguin/GeorgianWritingWizard/data/stripped_data.txt'\n",
    "cc = '/home/penguin/GeorgianWritingWizard/data/out.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cc, 'w', encoding='utf-8') as out_file:\n",
    "    with open(pp, 'rb') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = line.decode('utf-8')\n",
    "                out_file.write(data.strip() + '\\n')\n",
    "            except Exception as e:\n",
    "                raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('text', data_files=['/home/penguin/GeorgianWritingWizard/data/whole_corpus/georgian_large_corpus_cl2.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sml_data = data['train'].select(range(3_000_000, 3_000_000 + batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = load_dataset('text', '/home/penguin/GeorgianWritingWizard/data/whole_corpus/filter_v2/filtered.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "text_column_name = 'text'\n",
    "tokenizer = AutoTokenizer.from_pretrained('<path>', model_max_length=512)\n",
    "trash_cutoff = 0.3\n",
    "\n",
    "\n",
    "def filtering_rule(examples):\n",
    "    tokenized = tokenizer(examples[text_column_name])[\"input_ids\"]\n",
    "    return [len(t) < trash_cutoff * len(e) for t, e in zip(tokenized, examples[text_column_name])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_setup = dict(\n",
    "    batched=True,\n",
    "    batch_size=1024,\n",
    "    num_proc=None,  # a bit messy but c4 in RAM can be overbearing otherwise\n",
    "    # load_from_cache_file=False,\n",
    "    # keep_in_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = new_dataset.filter(\n",
    "    filtering_rule,\n",
    "    desc=\"Filter sentences that cannot be tokessnized well.\",\n",
    "    **map_setup,\n",
    "    # keep_in_memory=True,  # can run out of mem even on the 750GB node?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.push_to_hub('ZurabDz/geo_small_corpus_dedublicated_trash_off', token='<>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.push_to_hub('ZurabDz/geo_small_corpus', token='<huggingface token>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('ZurabDz/geo_small_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/penguin/GeorgianWritingWizard/language_engine/examples/outputs/2023-03-09/22-36-38/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub('ZurabDz/GeoSentencePieceBPE_32768_v2', use_auth_token='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')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31bef8f0e01d86822998708cae530d3f279653869977ee7676ea73d628f47b0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
